{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09b5c308",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26220\\3960947783.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mllm_classifier_dataset\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLLMClassifierDatabase\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import svm\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import gensim\n",
    "from gensim.models import fasttext\n",
    "import shap\n",
    "import re\n",
    "\n",
    "\n",
    "from src.dataset.llm_classifier_dataset import LLMClassifierDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfebcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns predictions on test data using LogReg model\n",
    "def generateLogisticRegression(data, classes, predictMe):\n",
    "    model = LogisticRegression(penalty='l2', max_iter = 250).fit(data, classes)\n",
    "    return model.predict(predictMe)\n",
    "    \n",
    "#returns predictions on test data using Naive Bayes model\n",
    "def generateNaiveBayes(data, classes, predictMe):\n",
    "    model = GaussianNB().fit(data,classes)\n",
    "    return model.predict(predictMe)\n",
    "    \n",
    "#prec, f1, recall, aucroc, returns all 4 in tuple\n",
    "def runMetrics(predicted, actual):\n",
    "    prec = sk.metrics.precision_score(actual, predicted)\n",
    "    f1 = sk.metrics.f1_score(actual, predicted)\n",
    "    recall = sk.metrics.recall_score(actual, predicted)\n",
    "    aucroc = sk.metrics.roc_auc_score(actual, predicted)\n",
    "    print(\"Precision score of the model:\", prec)\n",
    "    print(\"F1 score of the model\", f1)\n",
    "    print(\"Recall score of the model\", recall)\n",
    "    print(\"AUCROC of the model\", aucroc)\n",
    "    return (prec, f1, recall, aucroc)\n",
    "\n",
    "#to test the iris data set, wont be used on real dataset\n",
    "def runMetricsMulticlass(predicted, actual):\n",
    "    prec = sk.metrics.precision_score(actual, predicted, average = 'micro')\n",
    "    f1 = sk.metrics.f1_score(actual, predicted, average = 'micro')\n",
    "    recall = sk.metrics.recall_score(actual, predicted, average = 'micro')\n",
    "    print(\"Precision score of the model:\", prec)\n",
    "    print(\"F1 score of the model\", f1)\n",
    "    print(\"Recall score of the model\", recall)\n",
    "    return [prec, f1, recall]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86976778",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LLMClassifierDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26220\\867635200.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mdbdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLLMClassifierDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdb_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"path/to/database/file.db\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mload_to_memory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvectorize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[0mdbdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdbdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mallData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LLMClassifierDataset' is not defined"
     ]
    }
   ],
   "source": [
    "model_path = 'C:/Users/alex/Documents/MLFolder/wiki-news-300d-1M.vec'\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=False)\n",
    "\n",
    "#data should be [[[sent vect][sent vect]],[list of sent in response],[list of sent in response],response,response], \n",
    "# where n is number of responses, X1 is number of sentences in response, X2 is word vector\n",
    "def runfasttext(data):\n",
    "    fasttextout = []#will need to be an array :*(\n",
    "    for i in range(len(data)):\n",
    "        fasttextout.append(re.split('.?!', data[i]))#split across sentences\n",
    "        for j in range(len(fasttextout[i])):\n",
    "            fasttextout[i][j] = model.infer_vector(fasttextout[i][j])\n",
    "    return fasttextout\n",
    "    \n",
    "#data should be [[[sent vect][sent vect]],[list of sent in response],[list of sent in response],response,response], \n",
    "#where n is number of responses, X1 is number of sentences in response, X2 is word vector\n",
    "#result of this should hopefully standardize the number of sent vect per response, \n",
    "#so the array is (n,max_seq_length,gensim.model.vector_size)\n",
    "def padInput(data):   \n",
    "    max_seq_length = max(len(seq) for seq in data)\n",
    "    padData = np.array([])\n",
    "    for i in range(len(data)):\n",
    "        #amount of missing sentence vectors\n",
    "        fixedColumn = np.array(data[i]).reshape(-1,len(data[i][0]))\n",
    "        \n",
    "        #we are padding j times, so that len(fixedColumn) = max_seq\n",
    "        for j in range(max_seq_length - len(data[i])):\n",
    "            fixedColumn = np.append(np.array(data[i]),np.zeros(gensim.model.vector_size)).reshape(-1,gensim.model.vector_size)\n",
    "        padData = np.append(padData, fixedColumn).reshape(-1,len(fixedColumn),gensim.model.vector_size)\n",
    "    return padData\n",
    "\n",
    "\n",
    "dbdata = LLMClassifierDataset(db_path=\"path/to/database/file.db\", load_to_memory=True, vectorize=True)\n",
    "dbdata = dbdata.tolist()\n",
    "allData = np.array()\n",
    "allLabels = np.array()\n",
    "for i in range(len(dbdata)):\n",
    "    allData = np.append(allData,dbdata[i][0])\n",
    "    allLabels = np.append(allLabels,dbdata[i][1])\n",
    "    \n",
    "#this should make a marco and llm dataset\n",
    "#will have N elements, each element will be the prompt string with the answer strong attatched at the end\n",
    "\n",
    "allData = fasttext(allData)\n",
    "allData = padInput(allData)\n",
    "p = np.random.permutation(len(a))\n",
    "allData = allData[p]\n",
    "allLabels = allLabels[p]\n",
    "trainData = allData[0:4*len(allData)/5]\n",
    "trainLabels = allLabels[0:4*len(allData)/5]\n",
    "testData = allData[4*len(allData)/5:len(allData)]\n",
    "testLabels = allLabels[4*len(allData)/5:len(allData)]\n",
    "\n",
    "predClassLog = generateLogisticRegression(trainData,trainLabels,testData)\n",
    "predClassBayes = generateNaiveBayes(trainData,trainLabels,testData)\n",
    "runMetrics(predClassLog,testLabels)\n",
    "runMetrics(predClassBayes,testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8d8bbb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[2 7]\n",
      "  [9 1]\n",
      "  [1 3]]\n",
      "\n",
      " [[5 6]\n",
      "  [6 1]\n",
      "  [1 1]]]\n",
      "\n",
      "[[[ 1  3]\n",
      "  [ 2  4]\n",
      "  [ 4  8]]\n",
      "\n",
      " [[ 2 10]\n",
      "  [ 9 11]\n",
      "  [11  3]]]\n"
     ]
    }
   ],
   "source": [
    "testing = np.array([[[2,7],[9,1],[1,3]],[[5,6],[6,1],[1,1]],[[1,3],[2,4],[4,8]],[[2,10],[9,11],[11,3]]])\n",
    "besting = np.array([])\n",
    "for i in range(len(testing)):\n",
    "    fixedColumn = np.array(testing[i]).reshape(-1,len(testing[i][0]))\n",
    "    fixedColumn = np.append(fixedColumn,np.array([0,0])).reshape(-1,len(testing[0][0]))\n",
    "    # print(fixedColumn)\n",
    "    # print(\"-------\")\n",
    "    besting = np.append(besting, fixedColumn).reshape(-1,len(fixedColumn),len(testing[0][0]))\n",
    "\n",
    "print(testing[0:2])\n",
    "print(\"\")\n",
    "print(testing[2:4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f16e3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iris data to make sure models do stuff\n",
    "xData, yData = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(xData, yData)#once data is built\n",
    "\n",
    "(X_train, X_test) = padInput(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "210e32a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4688d259",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0702e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict, then run metrics\n",
    "predsLogReg = generateNaiveBayes(X_train, y_train, X_test)\n",
    "predsBayes = generateLogisticRegression(X_train, y_train, X_test)\n",
    "print(predsLogReg)\n",
    "print(predsBayes)\n",
    "print(y_test)\n",
    "runMetricsMulticlass(predsLogReg, y_test)\n",
    "runMetricsMulticlass(predsBayes, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e77d219",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating custom data set with 2 classes to test shap values. Naive bayes and log reg will struggle as \n",
    "#this dataset is completlty random :)\n",
    "\n",
    "n = 1000\n",
    "genData = np.random.rand(n, 8)\n",
    "genClasses = (np.random.rand(n,)+.5).astype(int)/1\n",
    "\n",
    "X_gen_train, X_gen_test, y_gen_train, y_gen_test = train_test_split(genData, genClasses)\n",
    "\n",
    "predsGenLogReg = generateNaiveBayes(X_gen_train, y_gen_train, X_gen_test)\n",
    "predsGenBayes = generateLogisticRegression(X_gen_train, y_gen_train, X_gen_test)\n",
    "\n",
    "runMetrics(predsGenLogReg, y_gen_test)#should preform poorly since the data is randomized\n",
    "print(\"\")\n",
    "runMetrics(predsGenBayes, y_gen_test)#should preform poorly since the data is randomized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0bb85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logRegModel = LogisticRegression(penalty='l2', max_iter = 250).fit(X_gen_train, y_gen_train)\n",
    "explainer = shap.LinearExplainer(logRegModel, X_gen_train)\n",
    "shap_values = explainer.shap_values(X_gen_train)\n",
    "bapValues = explainer(X_gen_test)\n",
    "    \n",
    "shap.initjs()\n",
    "shap.plots.beeswarm(bapValues) #doesn't seem to work with multiclass data. Shouldn't be a problem for our application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ae23e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "marcoData = np.random.rand(500, 4) + .3 #placeholder. Will be replaced with vectorized marco responses\n",
    "LLMData = np.random.rand(500, 4)#placeholder. Will be replaced with vectorized LLM resposnes\n",
    "\n",
    "#------\n",
    "#wrangling of MARCO data will be here\n",
    "\n",
    "#------\n",
    "#wrangling of LLM data will be here\n",
    "\n",
    "marcoLabels = np.zeros(len(marcoData))\n",
    "LLMLabels = np.ones(len(LLMData))\n",
    "\n",
    "realModel = (np.append(marcoData, LLMData)).reshape(len(marcoData)+len(LLMData), len(marcoData[0]))\n",
    "realClasses = np.append(marcoLabels, LLMLabels)\n",
    "\n",
    "p = np.random.permutation(len(realModel)) #give it a good shuffle :D\n",
    "realModel = realModel[p]\n",
    "realClasses = realClasses[p]\n",
    "\n",
    "X_response_train, X_response_test, y_origin_train, y_origin_test = train_test_split(realModel, realClasses)\n",
    "\n",
    "predictedLogReg = generateLogisticRegression(X_response_train,y_origin_train,X_response_test)\n",
    "predictedBayes = generateNaiveBayes(X_response_train,y_origin_train,X_response_test)\n",
    "\n",
    "#should be able to distinguish them decently well. Some data overlap will prevent it from getting super high\n",
    "#but it should realize higher values = fake marco dataset, lower values = fake llm dataset\n",
    "print(\"logistic metrics on real dataset:\")\n",
    "runMetrics(y_origin_test,predictedLogReg)\n",
    "print(\"\\naive bayes on real dataset:\")\n",
    "runMetrics(y_origin_test,predictedBayes)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
